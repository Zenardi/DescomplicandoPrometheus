# Descomplicando o Prometheus

## DAY-5

### O que iremos ver hoje?

<details>
<summary class="summary">DAY-5</summary>

- [Descomplicando o Prometheus](#descomplicando-o-prometheus)
  - [DAY-5](#day-5)
    - [O que iremos ver hoje?](#o-que-iremos-ver-hoje)
    - [Configurando o prometheus para criar nosso primeiro alerta](#configurando-o-prometheus-para-criar-nosso-primeiro-alerta)
    - [Vendo os alertas no Grafana](#vendo-os-alertas-no-grafana)
    - [Configurando contact points o Grafana](#configurando-contact-points-o-grafana)
    - [Criando mais alertas no Prometheus](#criando-mais-alertas-no-prometheus)
      - [1. Disponibilidade da Inst√¢ncia](#1-disponibilidade-da-inst√¢ncia)
      - [2. CPU (Utiliza√ß√£o e Satura√ß√£o)](#2-cpu-utiliza√ß√£o-e-satura√ß√£o)
      - [3. Mem√≥ria (Disponibilidade)](#3-mem√≥ria-disponibilidade)
      - [4. Disco / Filesystem (Espa√ßo e Previs√£o)](#4-disco--filesystem-espa√ßo-e-previs√£o)
      - [5. Rede (Erros e Tr√°fego)](#5-rede-erros-e-tr√°fego)
      - [Depois de atualizar o arquivo de alertas](#depois-de-atualizar-o-arquivo-de-alertas)
    - [Criando Dashboads com os alertas do Prometheus](#criando-dashboads-com-os-alertas-do-prometheus)
    - [O que √© o AlertManager](#o-que-√©-o-alertmanager)
      - [1. Desduplica√ß√£o (Deduplication)](#1-desduplica√ß√£o-deduplication)
      - [2. Agrupamento (Grouping)](#2-agrupamento-grouping)
      - [3. Roteamento (Routing)](#3-roteamento-routing)
      - [4. Inibi√ß√£o (Inhibition)](#4-inibi√ß√£o-inhibition)
      - [Novo servi√ßo no docker-compose](#novo-servi√ßo-no-docker-compose)
      - [Exemplo visual de configura√ß√£o (`alertmanager.yml`)](#exemplo-visual-de-configura√ß√£o-alertmanageryml)
    - [Conhecendo um pouco mais sobre o AlertManager](#conhecendo-um-pouco-mais-sobre-o-alertmanager)
      - [Entendendo a configura√ß√£o](#entendendo-a-configura√ß√£o)
      - [Diferentes rotas e receivers](#diferentes-rotas-e-receivers)

</details>


### Configurando o prometheus para criar nosso primeiro alerta

Alerta para saber quantidade de bananas
```yaml
groups:
  - name: Meus primeiros alertas
    rules:
    - alert: AcabandoBanana
      expr: banana < 2
      annotations: 
        title: 'Acabando as bananas em casa {{ $labels.casa }}' 
        description: 'Urgente, temos somente {{ $value }} em {{ $labels.casa }}'
        value: {{ $value }}
      labels:
        severity: 'critical'
```

Ficando mais serio...

```yaml
groups:
- name: Basic monitoring
  rules:
  - alert: InstanceDown
    # Condition for alerting
    expr: up == 0
    for: 1m
    # Annotation - additional informational labels to store more information
    annotations:
      title: 'Instance {{ $labels.instance }} down'
      description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute.'
    # Labels - additional labels to be attached to the alert
    labels:
      severity: 'critical'
```

&nbsp;
&nbsp;

### Vendo os alertas no Grafana
Os alertas configurados no Prometheus ficam visiveis no Grafana!

![](./images/alertas-no-grafana.png)

&nbsp;
&nbsp;

### Configurando contact points o Grafana
Podemos configurar multiplos pontos de contato! e tambem colocar nao apenas um email, mas um grupo de email e configurar multitplos destinos alem do email (eg: teams, discod, jira, etc)

![](./images/contact-points.png)



&nbsp;
&nbsp;

### Criando mais alertas no Prometheus

```yaml
groups:
- name: Basic monitoring
  rules:
  - alert: InstanceDown
    # Condition for alerting
    expr: up == 0
    for: 1m
    # Annotation - additional informational labels to store more information
    annotations:
      title: 'Instance {{ $labels.instance }} down'
      description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute.'
    # Labels - additional labels to be attached to the alert
    labels:
      severity: 'critical'
  - alert: high_load
    expr: node_load1 > 0.5
    annotations:
      description: '{{ $labels.instance }} of job {{ $labels.job }} is under high load.'
      summary: Instance {{ $labels.instance }} under high load
      value: '{{ $value }}'
  
  - alert: disk_space
    expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) * 100 / node_filesystem_size_bytes > 5
    labels:
      instance: '{{ $labels.instance }}:{{ $labels.mountpoint }}'
    annotations:
      value: '{{ humanize $value }}%'
```

O `node_load1` √© uma m√©trica coletada e exposta pelo Node Exporter;
Aqui est√° o que ela significa detalhadamente:
* **O que mede**: Representa a m√©dia de carga do sistema (Load Average) no √∫ltimo 1 minuto.
* O **"Load Average"** indica quantos processos est√£o usando a CPU ou aguardando para usar a CPU (ou aguardando I/O de disco) naquele momento.
* Existem tamb√©m o node_load5 (m√©dia de 5 minutos) e node_load15 (m√©dia de 15 minutos).

```yaml
expr: node_load1 > 0.5
```

Isso significa que o alerta high_load ser√° disparado se a m√©dia de carga de 1 minuto for maior que 0.5. O Prometheus est√° verificando a cada intervalo se o servidor est√° "ocupado demais" (carga acima de 0.5) neste exato momento (janela de 1 minuto). Se estiver, ele dispara o alerta.

Para criar alertas eficazes com o **Node Exporter**, √© recomend√°vel seguir o **M√©todo USE** (Utiliza√ß√£o, Satura√ß√£o e Erros). Isso evita a "fadiga de alertas" e foca em sintomas reais que afetam o sistema.

Aqui est√£o as m√©tricas essenciais divididas por recurso, com exemplos de consultas PromQL para seus alertas.

#### 1. Disponibilidade da Inst√¢ncia

O alerta mais b√°sico e cr√≠tico. Indica se o Node Exporter (e consequentemente o servidor) est√° acess√≠vel.

* **M√©trica:** `up`
* **Quando alertar:** Quando o valor for `0`.
* **Exemplo PromQL:**
```promql
up{job="node_exporter"} == 0

```


*(Dica: Configure um `for: 5m` para evitar alertas em reboots r√°pidos).*

---

#### 2. CPU (Utiliza√ß√£o e Satura√ß√£o)

N√£o olhe apenas para o uso total; o *Load Average* √© vital para entender se h√° processos aguardando CPU (satura√ß√£o).

* **Uso de CPU Elevado:**
* **M√©trica:** `node_cpu_seconds_total`
* **L√≥gica:** Calcula a porcentagem de tempo que a CPU **n√£o** est√° ociosa (idle).
* **Exemplo PromQL (Uso > 90%):**
```promql
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90

```

* **Load Average Alto:**
* **M√©trica:** `node_load1` (ou `node_load5`, `node_load15`)
* **L√≥gica:** O Load deve ser comparado ao n√∫mero de CPUs. Se o Load for maior que a contagem de n√∫cleos, o sistema est√° saturado.
* **Exemplo PromQL:**
```promql
node_load1 > (count by (instance) (node_cpu_seconds_total{mode="idle"})) * 2

```


*(Alerta se o Load for o dobro do n√∫mero de CPUs).*



---

#### 3. Mem√≥ria (Disponibilidade)

O Linux usa mem√≥ria livre para cache, ent√£o monitorar "mem√≥ria livre" (`MemFree`) gera falsos positivos. O correto √© monitorar a **mem√≥ria dispon√≠vel** (`MemAvailable`), que inclui a mem√≥ria que pode ser reclamada de caches e buffers.

* **M√©trica:** `node_memory_MemAvailable_bytes` e `node_memory_MemTotal_bytes`
* **Quando alertar:** Quando a mem√≥ria dispon√≠vel for menor que 10% do total.
* **Exemplo PromQL:**
```promql
(node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 < 10
```

---

#### 4. Disco / Filesystem (Espa√ßo e Previs√£o)

Existem dois tipos de alertas aqui: "disco cheio agora" e "disco vai encher em breve".

* **Disco Cheio (%):**
* **M√©trica:** `node_filesystem_avail_bytes`
* **L√≥gica:** Exclua sistemas de arquivos virtuais ou de leitura (como tmpfs).
* **Exemplo PromQL (Menos de 10% livre):**
```promql
(node_filesystem_avail_bytes{fstype!=""} / node_filesystem_size_bytes{fstype!=""} * 100) < 10

```

* **Previs√£o de Disco Cheio (4 horas):**
* **L√≥gica:** Usa a fun√ß√£o `predict_linear` para projetar se o disco encher√° nas pr√≥ximas 4 horas com base na tend√™ncia da √∫ltima hora.
* **Exemplo PromQL:**
```promql
predict_linear(node_filesystem_avail_bytes{fstype!=""}[1h], 4 * 3600) < 0

```

---

#### 5. Rede (Erros e Tr√°fego)

Geralmente n√£o se alerta sobre *throughput* (banda) a menos que voc√™ tenha limites r√≠gidos, mas **erros** s√£o indicativos de falhas f√≠sicas ou de configura√ß√£o.

* **Erros na Interface:**
* **M√©trica:** `node_network_receive_errs_total` e `node_network_transmit_errs_total`
* **Exemplo PromQL (Aumento na taxa de erros):**
```promql
rate(node_network_receive_errs_total[5m]) > 0

```

**Resumo**

| Recurso | M√©trica Chave | Objetivo do Alerta |
| --- | --- | --- |
| **Geral** | `up` | Servidor offline (Down) |
| **CPU** | `node_cpu_seconds_total` | CPU saturada (>90%) |
| **Mem√≥ria** | `node_memory_MemAvailable_bytes` | Risco de OOM (Out of Memory) |
| **Disco** | `node_filesystem_avail_bytes` | Disco cheio ou prestes a encher |
| **System** | `node_time_seconds` | Drift de rel√≥gio (Time drift) |

Adicionamos labels de severidade (warning vs. critical) e annotations descritivas para que, quando o alerta chegar no Slack ou PagerDuty (via Alertmanager), voc√™ saiba exatamente o que est√° acontecendo.

```yaml
groups:
  - name: node_exporter_alerts
    rules:
      # 1. Inst√¢ncia Offline
      - alert: HostDown
        expr: up{job="node_exporter"} == 0
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Host {{ $labels.instance }} est√° offline"
          description: "O alvo do Node Exporter caiu h√° mais de 3 minutos."

      # 2. CPU: Uso alto (> 90%)
      - alert: HostHighCpuLoad
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Uso de CPU alto no host {{ $labels.instance }}"
          description: "A utiliza√ß√£o da CPU est√° acima de 90% nos √∫ltimos 5 minutos."

      # 3. Mem√≥ria: Menos de 10% dispon√≠vel
      - alert: HostOutOfMemory
        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Mem√≥ria baixa no host {{ $labels.instance }}"
          description: "O host tem menos de 10% de mem√≥ria dispon√≠vel."

      # 4. Disco: Menos de 10% de espa√ßo livre
      # O filtro fstype exclui sistemas de arquivos virtuais para evitar falsos positivos
      - alert: HostDiskSpaceLow
        expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs|squashfs|vfat"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs|squashfs|vfat"} * 100) < 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Disco quase cheio no host {{ $labels.instance }}"
          description: "O sistema de arquivos {{ $labels.mountpoint }} tem menos de 10% de espa√ßo livre."

      # 5. Disco: Previs√£o de enchimento em 4 horas
      - alert: HostDiskWillFillIn4Hours
        expr: predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs|squashfs|vfat"}[1h], 4 * 3600) < 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Disco encher√° em breve no host {{ $labels.instance }}"
          description: "Baseado na tend√™ncia da √∫ltima hora, o disco {{ $labels.mountpoint }} ficar√° sem espa√ßo nas pr√≥ximas 4 horas."

      # 6. Rede: Alta taxa de erros (Rx ou Tx)
      - alert: HostNetworkErrors
        expr: rate(node_network_receive_errs_total[5m]) > 0 or rate(node_network_transmit_errs_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Erros de rede no host {{ $labels.instance }}"
          description: "Detectada taxa de erros na interface de rede nos √∫ltimos 5 minutos."
```

&nbsp;
&nbsp;

#### Depois de atualizar o arquivo de alertas
Aqui esta a nova pagina de alertas depois de aplicado a configura√ß√£o de `files/alerts-completo.yaml`

```yaml
# docker-compose.yml

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts.yml:/etc/prometheus/alerts.yml
      - ./alerts-completo.yaml:/etc/prometheus/alerts-completo.yaml # <----
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - monitoring
    depends_on:
      - python-exporter
      - go-exporter
      - node-exporter
# ...
# -----------------

# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files: 
  - alerts.yml
  - alerts-completo.yaml # <---- Adicionado o novo arquivo de alertas nas configs do Prometheus

alerting:
  alertmanagers:
  - static_configs:
    - targets:
      - localhost:9093
    
scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]

  - job_name: "python-exporter"
    static_configs:
      - targets: ["python-exporter:8899"]
  
  - job_name: 'go-exporter'
    static_configs:
      - targets: ['go-exporter:7788']
  
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
```

![prom-alertas-completo](./images/prom_alertas_completo.png)


E os mesmos alertas ficam integrados no Grafana!!

![grafana-alertas-completo](./images/grafana_alertas_completo.png)

&nbsp;

![grafana-alertas-completo](./images/grafana_alertas_completo2.png)


&nbsp;
&nbsp;

### Criando Dashboads com os alertas do Prometheus

![](./images/grafana_dashboard_alerts.png)

&nbsp;
&nbsp;

### O que √© o AlertManager
Pense no **Alertmanager** como o "gerente de tr√°fego" ou a "central de triagem" dos seus alertas.

Enquanto o **Prometheus** √© o c√©rebro que calcula as m√©tricas e decide *se* algo est√° errado (disparando o alerta), o **Alertmanager** decide *o que fazer* com esse alerta depois que ele √© disparado.

Se voc√™ conectar o Prometheus direto no seu e-mail sem o Alertmanager, voc√™ receber√° 500 e-mails repetidos dizendo "Servidor Down" a cada minuto. O Alertmanager resolve isso.

Aqui est√£o as 4 fun√ß√µes vitais que ele desempenha:

#### 1. Desduplica√ß√£o (Deduplication)

Se o Prometheus verificar a cada 15 segundos que o disco est√° cheio, ele enviar√° um sinal de alerta a cada 15 segundos. O Alertmanager entende que √© o mesmo problema e **envia apenas uma notifica√ß√£o** inicial, em vez de spammar seu Slack ou E-mail.

#### 2. Agrupamento (Grouping)

Imagine que uma rede inteira caiu. Isso faria com que 50 servi√ßos diferentes disparassem alertas de "Inacess√≠vel" simultaneamente.

* **Sem Alertmanager:** Voc√™ recebe 50 notifica√ß√µes separadas no celular. P√¢nico total.
* **Com Alertmanager:** Ele "segura" os alertas por alguns segundos, percebe que s√£o relacionados (ex: todos v√™m do mesmo `cluster=producao`) e envia **uma √∫nica notifica√ß√£o** listando todos os servi√ßos afetados.

#### 3. Roteamento (Routing)

Ele decide **quem** recebe o alerta com base nas *labels*:

* Alertas com `severity: critical` -> Vai para o **PagerDuty** (acorda o engenheiro de plant√£o).
* Alertas com `team: database` -> Vai para o canal **#dba-alerts** no Slack.
* Alertas com `severity: info` -> Vai apenas para um e-mail de log ou nem notifica.

#### 4. Inibi√ß√£o (Inhibition)

Se o alerta **"Datacenter pegando fogo"** j√° est√° ativo, n√£o faz sentido receber alertas de **"Servidor fora do ar"** ou **"Banco de dados lento"** desse mesmo datacenter. O Alertmanager silencia os alertas "menores" se um alerta "pai" cr√≠tico j√° estiver disparado.

---

#### Novo servi√ßo no docker-compose

```yml
# ---
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
    networks:
      - monitoring
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
# ---
```

#### Exemplo visual de configura√ß√£o (`alertmanager.yml`)

A configura√ß√£o funciona como uma √°rvore de decis√£o. Aqui est√° um exemplo simplificado:

```yaml
route:
  # Rota padr√£o (se nenhuma outra regra der match)
  receiver: 'email-padrao'
  group_wait: 30s
  group_interval: 5m

  routes:
    # Se for CRITICO, manda para o PagerDuty
    - match:
        severity: critical
      receiver: 'pagerduty-ops'

    # Se for do time de FRONTEND, manda para o Slack deles
    - match:
        team: frontend
      receiver: 'slack-frontend'

receivers:
- name: 'pagerduty-ops'
  pagerduty_configs:
  - service_key: <sua-chave-aqui>

- name: 'slack-frontend'
  slack_configs:
  - api_url: <sua-webhook-url>
    channel: '#frontend-alerts'

- name: 'email-padrao'
  email_configs:
  - to: 'admin@empresa.com'

```
&nbsp;
&nbsp;

**Resumo**

O Prometheus diz: *"Ei, tem um problema aqui!"*

O Alertmanager diz: *"Ok, j√° ouvi. Vou avisar o Fulano no Slack, mas s√≥ uma vez. Se n√£o resolver em 4 horas, eu aviso o Chefe."*

&nbsp;
&nbsp;


### Conhecendo um pouco mais sobre o AlertManager

Vamos configurar o AlertManager para que ele envie alertas para o Discord.

**Passo 1: Obter os Webhooks**

Antes de mexer na configura√ß√£o, voc√™ precisa dos links "destinos" onde o Alertmanager vai bater.

**Discord:**
* V√° nas configura√ß√µes do servidor onde quer receber os alertas (clicando no nome do servidor e indo em Server Settings).
* V√° em **Integra√ß√µes** > **Webhooks**.
* Clique em "Novo Webhook", d√™ um nome (ex: "Prometheus") e copie a **URL do Webhook**.



---

**Passo 2: O arquivo template `alertmanager-discord.template.yml`**

Substitua `COLE_SUA_URL_DO_WEBHOOK_DO_DISCORD_AQUI` pela webbook do Discord.

```yaml
global:
  resolve_timeout: 5m

route:
  # Agrupa alertas pelo nome (ex: todos os "HostDown" v√™m juntos em uma msg)
  group_by: ['alertname']
  
  # Espera 30s antes de mandar o primeiro alerta para ver se outros similares aparecem
  group_wait: 30s
  
  # Se novos alertas do mesmo grupo aparecerem, espera 5m antes de mandar atualiza√ß√£o
  group_interval: 5m
  
  # Se o problema persistir, reenvia a notifica√ß√£o a cada 4 horas
  repeat_interval: 4h
  
  # Destino padr√£o
  receiver: 'discord-webhook'

receivers:
  - name: 'discord-webhook'
    discord_configs:
      - webhook_url: '__DISCORD_WEBHOOK_URL__'
        
        # T√≠tulo din√¢mico: Muda √≠cone e status (FIRING vs RESOLVED)
        title: '{{ if eq .Status "firing" }}üî•{{ else }}‚úÖ{{ end }} [{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}'
        
        # Mensagem formatada
        message: >-
          **Severidade:** {{ .CommonLabels.severity | toUpper }}
          
          **Resumo:** {{ .CommonAnnotations.summary }}
          
          **Detalhes:** {{ .CommonAnnotations.description }}
          
          **Atingido:** {{ range .Alerts }}`{{ .Labels.instance }}` {{ end }}

```

&nbsp;
&nbsp;

#### Entendendo a configura√ß√£o

Aqui est√° o que cada parte dessa configura√ß√£o faz:

* **`global`**: Configura√ß√µes gerais.
    * `resolve_timeout`: Tempo que o Alertmanager aguarda antes de considerar um alerta resolvido se o Prometheus parar de enviar atualiza√ß√µes.

* **`route`**: √â o cora√ß√£o da l√≥gica de envio.
    * `group_by: ['alertname']`: Isso √© muito importante! Significa que o Alertmanager vai agrupar todos os alertas que tenham o mesmo nome em uma √∫nica notifica√ß√£o. Se 10 inst√¢ncias ca√≠rem (10 alertas `InstanceDown`), voc√™ recebe **uma** mensagem no Discord listando as 10, em vez de 10 mensagens separadas.
    * `group_wait: 30s`: Quando o primeiro alerta chega, ele espera 30 segundos "coletando" outros alertas do mesmo grupo antes de disparar. Isso ajuda a agrupar falhas simult√¢neas.
    * `group_interval: 5m`: Se novos alertas chegarem para esse mesmo grupo depois da primeira notifica√ß√£o, ele espera 5 minutos para enviar uma atualiza√ß√£o (evita flood se os alertas ficarem "piscando").
    * `repeat_interval: 4h`: Se o alerta continuar disparado (firing) e nada mudar, ele reenvia a notifica√ß√£o a cada 4 horas para lembrar que o problema ainda existe.
    * `receiver: 'discord-webhook'`: Define quem vai receber os alertas dessa rota.

* **`receivers`**: A lista de canais de comunica√ß√£o.
    * `discord_configs`: Configura√ß√£o espec√≠fica para o Discord.
    * `title` e `message`: Aqui usamos **Go Templates** para personalizar a mensagem.
        * `{{ .Status }}`: Mostra se est√° "firing" (fogo) ou "resolved" (resolvido).
        * `{{ .CommonLabels.severity }}`: Pega o label de severidade que definimos no Prometheus.
        * `{{ .CommonAnnotations.description }}`: Pega a descri√ß√£o do problema.
        * `{{ range .Alerts }}`: Loop para listar todas as inst√¢ncias afetadas na mesma mensagem.

Dentro de `files` crie um arquivo `.env` que contenha o seguinte conteudo, substituindo obviamente o valor desta variavel.

```sh
export DISCORD_WEBHOOK=[SEU_WEBHOOK_AQUI]

```

Antes de criar o alertmanager com docker compose, se certifique que voce executou `./.env` para expor esta variavel. Como este valor √© sensivel, n√£o √© legal ter isso o git.

Com o alertmanager no ar, podemos acessar os alertas pipocando!

![alertmanager](./images/alertmanager.png)

E pouco tempo depois, o AlertManager ir√° publicar uma mensagem no Discord!

![discord_alert](./images/discord_alert.png)


&nbsp;
&nbsp;

#### Diferentes rotas e receivers

O Alertmanager √© extremamente flex√≠vel. Voc√™ pode configurar para que alertas cr√≠ticos acordem algu√©m de madrugada (PagerDuty/Liga√ß√£o), enquanto alertas de aviso apenas enviem um e-mail ou mensagem no Slack durante o hor√°rio comercial.

Aqui est√° um exemplo completo e comentado de uma configura√ß√£o avan√ßada (`alertmanager.yml`) que utiliza:
1.  **Time Intervals:** Para definir hor√°rios comerciais.
2.  **Child Routes:** Rotas espec√≠ficas para certos alertas.
3.  **Inibi√ß√£o:** Para n√£o receber alertas de "Warning" se j√° existe um "Critical".
4.  **M√∫ltiplos Receivers:** Slack e E-mail.

```yaml
global: # Inicio da se√ß√£o global 
  resolve_timeout: 3m # Tempo de espera que o AlertManager aguarda para receber um update de um alerta do Prometheus...
  slack_api_url: ''

# 1. Definindo hor√°rios de trabalho
time_intervals: # Intervalos de tempo para serem usados nas rotas
  - name: workdays
    time_intervals:
      - weekdays: ['monday:friday']
  - name: workhours
    time_intervals:
      - times: 
        - start_time: 09:00
          end_time: 18:00

route: # Inicio da se√ß√£o route
  group_by: ['alertname']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 1h
  receiver: 'gmail-notifications' # Recebedor padr√£o se nenhuma regra abaixo der match

  routes: # Rotas filhas
    # Rota espec√≠fica para LoadAverage mandar no Slack
    # MAS apenas durante dias de semana e hor√°rio comercial!
    - receiver: 'slack'
      group_wait: 10s
      matchers:
        - alertname = LoadAverage
      active_time_intervals:
        - workdays
        - workhours
    
    # Rota para alertas de NodeDown
    - receiver: 'gmail-notifications'
      matchers:
        - alertname = NodeDown

# 3. Regras de Inibi√ß√£o
# Se houver um alerta "Critical", n√£o me envie notifica√ß√£o do "Warning" para o mesmo host.
inhibit_rules: 
  - source_match: 
      severity: 'critical'
    target_match: 
      severity: 'warning'
    equal: ['alertname', 'dev', 'instance']

receivers: # Recebedores de alertas
- name: 'slack'
  slack_configs:
  - channel: '#testing'
    send_resolved: true 
    title: |- 
     [{{ .Status | toUpper }}] {{ .CommonLabels.alertname }}
    text: >-
     {{ range .Alerts -}}
     *Alert:* {{ .Annotations.title }}
     {{ end }}

- name: 'gmail-notifications'
  email_configs:
  - to: jeferson@linuxtips.io
    from: jeferson@linuxtips.io
    smarthost: smtp.gmail.com:587
    # ... autentica√ß√£o omitida ...
```

**Pontos chave deste exemplo:**

*   **`active_time_intervals`**: O alerta de *LoadAverage* s√≥ ser√° enviado para o Slack se ocorrer entre Segunda e Sexta, das 09:00 √†s 18:00. Fora desse hor√°rio, ele cai na rota padr√£o (ou √© ignorado se a rota padr√£o for nula).
*   **`matchers`**: Permite filtrar alertas. Voc√™ pode usar `=` (igual) ou `=~` (regex). No exemplo, se `alertname = LoadAverage`, ele desvia para o Slack.
*   **`inhibit_rules`**: √â muito √∫til. Imagine que o disco encheu (Warning: 85%, Critical: 95%). Se chegar em 95%, voc√™ gera um alerta Critical. A regra de inibi√ß√£o silencia o alerta de Warning para que voc√™ n√£o fique recebendo duas notifica√ß√µes sobre o mesmo problema (o disco estar cheio).

---